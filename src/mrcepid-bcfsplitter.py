#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import dxpy
import subprocess
import csv
import json
import tarfile
import glob


# This function runs a command on an instance, either with or without calling the docker instance we downloaded
# By default, commands are not run via Docker, but can be changed by setting is_docker = True
def run_cmd(cmd: str, is_docker: bool = False) -> None:

    if is_docker:
        # -v here mounts a local directory on an instance (in this case the home dir) to a directory internal to the
        # Docker instance named /test/. This allows us to run commands on files stored on the AWS instance within Docker
        cmd = "docker run " \
              "-v /home/dnanexus:/test " \
              "egardner413/mrcepid-filtering " + cmd

    # Standard python calling external commands protocol
    print(cmd)
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()

    # If the command doesn't work, print the error stream and close the AWS instance out with 'dxpy.AppError'
    if proc.returncode != 0:
        print("The following cmd failed:")
        print(cmd)
        print("STDERROR follows\n")
        print(stderr.decode('utf-8'))
        raise dxpy.AppError("Failed to run properly...")


def split_sites(vcfprefix):

    # Do first iteration to get number of lines
    sites = csv.DictReader(open("bcf_sites.txt", "r"), delimiter="\t", fieldnames = ["chrom","pos"], quoting = csv.QUOTE_NONE)
    n_lines = 0
    for site in sites:
        n_lines += 1
    if n_lines % 5000 < 2500:
        append_last = True
    else:
        append_last = False

    # Then do second iteration to write the actual index files:
    coordinate_writer = open(vcfprefix + ".coordinates.tsv", "w")
    current_index_num = 1
    current_start = 0
    current_end = 0
    sites = csv.DictReader(open("bcf_sites.txt", "r"), delimiter="\t", fieldnames = ["chrom","pos"], quoting = csv.QUOTE_NONE)
    file_chunk_names = []
    current_index = open(vcfprefix + "_chunk" + str(current_index_num), "w")
    # file_chunk_names.append(vcfprefix + "_chunk" + str(current_index_num))
    for site in sites:
        if sites.line_num == 1:
            current_start = site['pos']
        elif (sites.line_num - 1) % 5000 == 0:
            print(sites.line_num)
            if (n_lines - sites.line_num) < 2500 and append_last is False:
                coordinate_writer.write("%s\t%s\t%s\t%s\n" % (site['chrom'], current_start, current_end, vcfprefix + "_chunk" + str(current_index_num)))
                current_index.close()
                current_index_num += 1
                current_index = open(vcfprefix + "_chunk" + str(current_index_num), "w")
                # file_chunk_names.append(vcfprefix + "_chunk" + str(current_index_num))
                current_start = site['pos']
            elif (n_lines - sites.line_num) < 2500 and append_last is True:
                print("Appending remaining lines to end of last chunk...")
            else:
                coordinate_writer.write("%s\t%s\t%s\t%s\n" % (site['chrom'], current_start, current_end, vcfprefix + "_chunk" + str(current_index_num)))
                current_index.close()
                current_index_num += 1
                current_index = open(vcfprefix + "_chunk" + str(current_index_num), "w")
                # file_chunk_names.append(vcfprefix + "_chunk" + str(current_index_num))
                current_start = site['pos']
        current_index.write("%s\t%s\n" % (site['chrom'], site['pos']))
        current_end = site['pos']

    current_index.close()
    coordinate_writer.write("%s\t%s\t%s\t%s\n" % (site['chrom'], current_start, current_end, vcfprefix + "_chunk" + str(current_index_num)))
    coordinate_writer.close()

    return file_chunk_names


def split_bcfs(vcfprefix, file_chunk_names):

    current_chunk = 1
    bcf_files = []
    for file in file_chunk_names:
        cmd = "bcftools view --threads 8 -T /test/" + file + " -Ob -o /test/" + vcfprefix + "_chunk" + str(current_chunk) + ".bcf /test/variants.vcf.gz"
        run_cmd(cmd, True)
        bcf_files.append(dxpy.upload_local_file(vcfprefix + "_chunk" + str(current_chunk) + ".bcf"))
        current_chunk += 1

    return bcf_files

@dxpy.entry_point('main')
def main(input_vcf, input_index):

    # Download the VCF file & index chunk to the instance
    vcf = dxpy.DXFile(input_vcf)
    dxpy.download_dxfile(vcf.get_id(), "variants.vcf.gz")
    vcfidx = dxpy.DXFile(input_index)
    dxpy.download_dxfile(vcfidx.get_id(), "variants.vcf.gz.tbi")

    # Set a prefix name for all files so that we can output a standard-named file:
    vcfprefix = vcf.describe()['name'].split(".vcf.gz")[0]

    # Bring a prepared docker image into our environment so that we can run commands we need:
    # The Dockerfile to build this image is located at resources/Dockerfile
    cmd = "docker pull egardner413/mrcepid-filtering:latest"
    run_cmd(cmd)

    # Do splitting of the target VCF:
    # First generate a list of all variants in the file
    cmd = "bcftools query -f \"%CHROM\\t%POS\\n\" -o /test/bcf_sites.txt /test/variants.vcf.gz"
    run_cmd(cmd, True)
    # Generate reasonable sites:
    file_chunk_names = split_sites(vcfprefix)
    # And split the files into chunks:
    bcf_files = split_bcfs(vcfprefix, file_chunk_names)

    # Set output
    output = {"output_vcfs": [dxpy.dxlink(item) for item in bcf_files]}

    return output

dxpy.run()
